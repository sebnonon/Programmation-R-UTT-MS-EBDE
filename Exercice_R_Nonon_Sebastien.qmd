---
title: "Exercice R - MS EBDE UTT 2025/2026 - Sébastien Nonon"
format: html
editor: visual
execute:
  warning: false
  message: false
---

# Module 1

On commence par nettoyer notre environnement:

```{r}
rm(list = ls())
gc()
```

## Exercice 1

Créer le vecteur qui contient tous les multiples de 3 entre 1 et 50.

Pour cela on utilise la fonction seq() pour créer notre vecteur.

On affiche ensuite notre vecteur avec la fonction print().

```{r}
notre_vec <- seq(from = 3, to = 50, by = 3)
print(notre_vec)
```

## Exercice 2

Créer la fonction "tronque()" qui prend pour argument un nombre x et un vecteur vec et qui change tous les éléments du vecteur vec supérieur à x en x.

On écrit la définition de la fonction:

```{r}
tronque <- function(x,vec){
  # On filtre vec sur les éléments supérieur à x, et on leur attribue x
  vec[vec>x] <- x
  return(vec)
}
```

On teste la fonction avec le vecteur de l'exercice 1, que l'on tronque à 30:

```{r}
print(tronque(30, notre_vec))
```

On est content on a bien tronqué notre vecteur.

## Exercice 3

Créer la fonction "maxi()" qui a un vecteur numérique associe son élément maximum.

```{r}
maxi <- function(vec){
  
  # on initialise le maximum avec le premier élément
  max <- vec[1]
  
  for (i in 2:length(vec)){ # on parcoure tous les éléments
    
    if (vec[i] > max){ # on compare avec notre max actuel
      
      max <- vec[i] # condition vérifiée alors on atrribue notre nouveau max
    }
  }
  return(max)
}
```

On teste notre focntion sur le vecteur du premier exo:

```{r}
print(maxi(notre_vec)) # on affiche le résultat
```

## Exercice 4

Créer un dataframe de 10 étudiants avec leur nom, prénom, âge, sexe, entreprise. Puis maniopulation sur le jeu de données.

On commence par créer notre dataframe:

```{r}
etudiants <- data.frame(
  nom = c("Dupont", "Jalibert", "Penaud", "Cros", "Jelonch",
          "Meafou", "Marchand", "Depoortere", "Flament", "Fickou"),
  prenom = c("Antoine", "Mathieu", "Damian", "François", "Anthony",
             "Emmanuel", "Julien", "Nicolas", "Thibault", "Gael"),
  age = c(18, 20, 19, 23, 22, 25, 17, 23, 24, 21),
  sexe = c("F", "M", "F", "M", "F", "M", "F", "M", "F", "M"),
  entreprise = c("Google", "Microsoft", "Amazon", "Apple", "Meta",
                 "IBM", "Oracle", "SAP", "Salesforce", "Facebook")
)
```

Notre DF est créée, on peut l'observer:

```{r}
print(etudiants)
```

On calcule l'âge moyen de notre groupe d'étudiant:

```{r}
print(mean(etudiants$age))
```

On regarde la proportion de femme dans notre groupe d'étudiant:

```{r}
nb_femme <- sum(etudiants$sexe == "F")
nb_total <- nrow(etudiants)
proportions_femme <- nb_femme / nb_total

print(proportions_femme)
```

On essaye maintenenat de créer une variable de tranche d'âge:

```{r}
etudiants$tranche_age <- cut(etudiants$age,
                             breaks = c(0,19,21,23,30),
                             labels = c("U19","U21","U23","A"))
```

On regarde si notre nouvelle variable s'est bien intégrée au dataframe:

```{r}
print(etudiants)
```

On va utiliser le package dplyr pour faire des statistiques par tranche d'âge, on commence donc par charger le package:

```{r}
library(dplyr)
```

On peut maintenant calculer l’âge moyen et la proportion de femmes par tranche d’âge.

```{r}
etudiants %>% 
  group_by(tranche_age) %>% 
  summarise(effectif = n(),
            age_moyen = round(mean(age),2),
            proportion_femmes = round(mean(sexe == "F"),2)) %>% 
  arrange(tranche_age)
```

# Module 2

On commence par nettoyer notre environnement:

```{r}
rm(list = ls())
gc()
```

## Exercice 1

On veut importer PisaFR et comparer avec PisaUS, on considère que nos données se trouvent dans le dossier "Data":

```{r}
pisa_fr <- read.table("Data/PisaFR.csv", 
                      header = TRUE, 
                      sep = ";", 
                      dec = ",", 
                      na = " ")

pisa_us <- read.table("Data/PisaUS.csv", 
                      header = TRUE, 
                      sep = ";", 
                      dec = ",", 
                      na = " ")
```

On observe les structures de PisaFR et de PisaUS:

```{r}
str(pisa_fr)
str(pisa_us)
```

On peut voir que nous avons les mêmes variables. Il y a plus de lignes d'observations dans PisaFR que dans PisaUS. Enfin, on retrouve la même variable GLCM remplie de NA dans les deux dataframes.

## Exercice 2

On supprime la colonne GLCM, qui est la 6e du dataframe:

```{r}
pisa_fr<-pisa_fr[,-6] # pour supprimer la sixième colonne
head(pisa_fr)
```

On veut supprimer toutes les observations dont la somme des notes en français (READ) et en maths (MATH) sont inférieure à 1000:

```{r}
pisa_fr <- pisa_fr[pisa_fr$READ + pisa_fr$MATH >= 1000, ]
```

Ainsi que celles des étudiants dont les notes en sciences (SCIE), sont inférieures à 500:

```{r}
pisa_fr <- pisa_fr[pisa_fr$SCIE >= 500, ]
```

On effectue un histogramme pour vérifier que notre filtre a bien fonctionné pour la condition des notes en sciences inférieures à 500:

```{r}
hist(pisa_fr$SCIE,
     main = "Distribution des notes en Sciences (SCIE)\nAprès filtrage (>= 500)",
     xlab = "Note en Sciences",
     ylab = "Fréquence",
     xlim = c(400, 900))
```

On est content on voit qu'il n'y a aucune observations avec une note inférieure à 500.

## Exercice 3

On souhaite refaire la question précèdente avec les packages dplyr et ggplot2.

Pour cela on commence par re-importer nos données:

```{r}
pisa_fr <- read.table("Data/PisaFR.csv", 
                      header = TRUE, 
                      sep = ";", 
                      dec = ",", 
                      na = " ")
```

On charge également les packages nécessaires:

```{r}
library(dplyr)
library(ggplot2)
```

Et maintenant nous pouvons utiliser les packages:

```{r}
pisa_fr %>%
  select(-GLCM) %>% 
  filter(READ + MATH >= 1000, SCIE >= 500) %>%
  ggplot(aes(x = SCIE)) +
  geom_histogram() +
  labs(
    title = "Distribution des notes en Sciences",
    x = "Note en Sciences",
    y = "Fréquence"
  ) 
```

## Exercice 4

On va fusionner nos deux dataframe:

```{r}
pisa_new <- rbind(pisa_fr, pisa_us)
```


On souhaite manipuler les données en utilisant les commandes slice, filter, select, relocate, rename, arrange, mutate, groupe_by et le pipe

```{r}
pisa_new %>%
  
  # renommer pour plus de clarté
  rename(Pays = COUNT, Sciences = SCIE, Math = MATH, Lecture = READ) %>%
  
  # créer une nouvelle variable
  mutate(Total_Score = Math + Lecture) %>%
  
  # filtrer les observations
  filter(Total_Score >= 1000, Sciences >= 500) %>%
  
  # sélectionner et réorganiser les colonnes
  select(Sciences, Math, Lecture, Total_Score, Pays) %>% 
  relocate(Total_Score, .after = Sciences) %>%
  
  # trier par score total décroissant
  arrange(desc(Total_Score)) %>%
  
  # grouper et résumer
  group_by(Pays) %>%
  
  # garder les 10 meilleurs
  slice(1:10)
```

## Exercice 5

On supprime tous les objets en mémoire:

```{r}
rm(list = ls())
```

Et on recharge les données de PisaFR:

```{r}
# Recharger les données
pisa_fr <- read.table("Data/PisaFR.csv", 
                      header = TRUE, sep = ";", dec = ",", na = " ")

library(tidyverse)
```

Grâce à la librairie dplyr nous allons regarder quelques statistiques clés pour les Maths, la Lecture et les Sciences:

```{r}
pisa_fr %>%
  summarise(
    Matières = c("Maths", "Lecture", "Sciences"),
    Moyenne = c(mean(MATH, na.rm = TRUE), mean(READ, na.rm = TRUE), mean(SCIE, na.rm = TRUE)),
    Mediane = c(median(MATH, na.rm = TRUE), median(READ, na.rm = TRUE), median(SCIE, na.rm = TRUE)),
    Ecart_Type = c(sd(MATH, na.rm = TRUE), sd(READ, na.rm = TRUE), sd(SCIE, na.rm = TRUE))
  )
```

On remarque que la dispersion est assez proche, les medianes et les écarts-types sont proches.

On décide de faire un boxplot pour chacune de ces matières car on s'intéresse à la dispersion de celles-ci:

```{r}
boxplot(pisa_fr[, c("MATH", "READ", "SCIE")],
        main = "Comparaison de la dispersion des notes",
        col = c("red", "blue", "green"),
        ylab = "Note")
```

Cela conforte l'impression que l'on a eu avec le summarise précedent.

Regardons maintenenat la corrélation entre les Maths et les Sciences, est-ce qu'il y a une relation ?

```{r}
ggplot(pisa_fr, aes(x = MATH, y = SCIE)) +
  geom_point() +
  labs(title = "Corrélation entre les Maths et les Sciences")
```

Visuellement on remarque une très forte corrélation positive entre les notes en maths et en sciences.

# Module 3

On commence par nettoyer notre environnement:

```{r}
rm(list = ls())
gc()
```

## Exercice 1

On importe les données:

```{r}
sa_heart <- read.table("data/SAHeart.csv", header = TRUE, sep = ",", dec = ".")
```

On observe les premières lignes de notre dataframe:

```{r}
head(sa_heart)
```

Ainsi que la structure du jeu de données:

```{r}
str(sa_heart)
```

## Exercice 2

Dans un premier temps nous allons conserver seulement les variables quantitatives:

```{r}
sa_heart_quant <- sa_heart %>%
  select(-famhist, -chd)
```

## Exercice 3

Nous allons transformer les données afin d'effectuer une ACP:

```{r}
sa_heart_cr <- scale(sa_heart_quant, center = TRUE, scale = TRUE)
```

On va utiliser les packages factoextra et FactoMineR pour réaliser l'ACP.

On charge nos librairies:

```{r}
library(factoextra)
library(FactoMineR)
```

On réalise l'ACP sur nos données centrées réduites:

```{r}
res.pca <- PCA(sa_heart_cr, graph = FALSE)
```

On peut à présent représenter le nuage des individus sur plan Axe1 - Axe2:

```{r}
# Nuage des individus
fviz_pca_ind(res.pca,
             geom = "point")    
```

On peut également regarder le nuage des variables:

```{r}
fviz_pca_var(res.pca, col.var = "black")
```

### Interprétation de l'axe 1

Cet axe est celui qui explique la plus grande part de la variance de nos données.

On observe que les variables adiposity, obesity, age et sbp pointent fortement vers la droite de l'axe.

### Interprétation de l'axe 2

Cet axe apporte une information complémentaire décorrélée du premier.

Les variables alcohol et tobacco pointent vers le haut, tandis que typea pointe vers le bas.

### Explicabilité des axes

D'après nos sorties, on a:
  - l'Axe 1 explique 35,1% de la variance totale des données
  - l'Axe 2 explique 15% de la variance
  - à eux deux, ces axes captent 50,1% de l'info

## Exercice 4

On veut colorer les points dans le nuage des individus en fonction des deux variables qualitatives.

### Selon CHD

On refait donc notre nuage des individus, en fonction de la présence de maladie cardiaque chez l'individu.

```{r}
fviz_pca_ind(res.pca,
             geom = "point",
             col.ind = as.factor(sa_heart$chd), # On transforme en facteur pour avoir des couleurs distinctes
             palette = c("blue", "red"), # Bleu pour sain (0), Rouge pour malade (1)
             legend.title = "Maladie (chd)")
```

Par rapport à l'axe 1, on remarque que les individus malades ont une valeur plus élevés. 

Ils ont donc des valeurs plus élevés sur les variables fortement représentées par cette axe (adiposity, obesity, age et sbp)

### Selon famhist

On refait donc notre nuage des individus, en fonction de la présence d'antécédents familiaux chez l'individu

```{r}
fviz_pca_ind(res.pca,
             geom = "point",
             col.ind = as.factor(sa_heart$famhist), # On transforme en facteur pour avoir des couleurs distinctes
             palette = c("blue", "red"), # Bleu pour sain (0), Rouge pour malade (1)
             legend.title = "Antécédents (famhist)")
```

On retrouve une disparité similaire à la variable chd.

## Exercice 5

Pour réaliser l'algo des k-means on reprend les données centrées et réduites.

On se fait un vecteur de stockage des inertie en fonciton du nb de cluster k.

```{r}
inertie_vec <- rep(0, times = 10)
```

On lance ensuite l'algo avec k allant de 1 à 10 clusters:

```{r}
for (k in 1:10) {
  # on utilise nstart=20 pour stabiliser les résultats
  km <- kmeans(sa_heart_cr, centers = k, nstart = 5)
  inertie_vec[k] <- km$tot.withinss
}
```

Et maintenant on peut faire un graphique pour regarder le critère du coude :

```{r}
plot(1:10, inertie_vec, type = "b", 
     xlab = "Nombre de clusters (k)", 
     ylab = "Inertie intra-classe",
     main = "Critère du coude pour SAHeart")
```

On observe un coude à k=3 donc on retient cette valeur pour notre paramètre.

```{r}
groupes.km <- kmeans(sa_heart_cr, centers = 3, nstart = 20)
```

Nous représentons maintenant les clusters créés sur le plan de l'axe1 - axe2 de l'ACP:

```{r}
fviz_cluster(groupes.km, data = sa_heart_cr, ellipse.type ="convex",
xlab = "Axe␣1", ylab = "Axe␣2")+
theme_minimal()
```

On ajoute la colonne des clusters (groupes.km$cluster) à notre jeu de données original pour pouvoir comparer.

```{r}
# Ajout des clusters au dataframe d'origine
sa_heart_bilan <- sa_heart %>%
  mutate(Cluster = as.factor(groupes.km$cluster))

# Tableau croisé pour la maladie (chd)
table(sa_heart_bilan$Cluster, sa_heart_bilan$chd)

# Tableau croisé pour les antécédents (famhist)
table(sa_heart_bilan$Cluster, sa_heart_bilan$famhist)
```

## Exercice 6

Nous allons effectuer une classification ascendante hiérarchique.

La première étape est de construire la matrice des distances avec la fonction dist:

```{r}
d.sa_heart <- dist(sa_heart_cr)
```

Ensuite on peut lancer le clustering hiérarchique avec la commande hclust() puis afficher le dendogramme:

```{r}
hc.ward = hclust(dist(sa_heart_cr), method="ward.D2")

plot(hc.ward)
```

À partir du dendrogramme, on peut construire les classes en effectuant une coupe horizontale dans l’arbre et récupérer les classes ainsi définies. La commande cutree permet de préciser le nombre de clusters que l’on veut et rect.hclust() de les représenter sur le dendogramme. 

Au préalable, pour nous aider à choisir le nombre de classes, nous pouvons représenter les sauts d’inertie du dendogramme selon le nombre de classes retenues.

```{r}
inertie <- sort(hc.ward$height, decreasing =  T)

plot(inertie[1:20], type = "s", xlab = "Nombre de classes", ylab = "Inertie")
```

On remarque un saut très net pour k = 2, ensuite on peut aussi relever un saut pour k = 5.

```{r}
clusterCAH = cutree(hc.ward,4)
plot(hc.ward)
rect.hclust(hc.ward, 2, border = "green3") # test avec 2 classes
rect.hclust(hc.ward, 5, border = "red")
```

Comme pour les kmeans, on peut afficher les clusters:

```{r}
fviz_cluster(list(cluster = clusterCAH, data = sa_heart_cr), 
             ellipse.type = "convex",
             xlab = "Axe 1", ylab = "Axe 2") +
  theme_minimal()
```

Comme pour les k-means, on croise ces nouveaux groupes avec chd et famhist.

```{r}
# Comparaison avec la maladie (chd)
table(clusterCAH, sa_heart$chd)
```

```{r}
# Comparaison avec les antécédents (famhist)
table(clusterCAH, sa_heart$famhist)
```

# Module 4

On nettoie l'environnement

```{r}
rm(list=ls())
```


## Partie 1 

### Exercice 1

Nous allons charger les données "Ozone.txt".

```{r}
ozone <- read.table("Data/ozone.txt", header = T, sep = " ", dec = ".")
```

On effectue quelques statistiques descriptives du jeu de données:

```{r}
summary(ozone)
```

On remarque un écart important entre le minimum et le maximum de max03. De plus, la moyenne est supérieur à la médiane de 10, ça peut indiquer la présence de pics de pollution.

On commence avec la matrice de corrélation pour voir quelles variables influent sur l'Ozone:

```{r}
# on utilise is.numeric pour garder les variables du df qui sont numériques, pour pouvoir appliquer la fonction cor

cor(ozone[, sapply(ozone, is.numeric)])
```

Pour essayer de mieux s'y retrouver dans la matrice de corrélation, on va utiliser le package ggcorrplot:

```{r}
library(ggcorrplot)

corr_matrix <- cor(ozone[, sapply(ozone, is.numeric)], use = "complete.obs")

ggcorrplot(corr_matrix, 
           lab = TRUE,        # Affiche les coefficients chiffrés sur la carte
           title = "Corrélations des variables de Ozone")
```
On observe ici une forte corrélation positive entre maxO3 et les températures, particulièrement T12 et T15. À l'inverse, une corrélation négative avec les variables Ne9, Ne12 ou Ne15.
On retrouve également de la colinéarité: les températures entre elles (T9, T12, T15) sont très fortement corrélées

### Exercice 2

On veut étudier la relation entre la variable maxO3 et T15 (la température à 15h). Pour cela on va effectuer une régression linéaire simple pour expliquer la concentration en ozone en fonction de cette température.

Le modèle utilisé est de la forme $maxO3 = \beta_0 + \beta_1 \cdot T15 + \epsilon$. On utilise la fonction lm() pour estimer les paramètres.

```{r}
# Réalisation de la régression linéaire simple
rl_simple <- lm(maxO3 ~ T15, data = ozone)

# Affichage des résultats détaillés
summary(rl_simple)
```

Le coefficicient $\beta_0 = -18.73$ est la valeur théorique de l'ozone si la température était de 0°C.
Le coefficient de $\beta_1 = 4.82$ indique l'augmentation moyenne de la concentration d'ozone pour chaque degré supplémentaire à 15h. Le coefficient est positif, cela confirme que la chaleur favorise la pollution.

Le R² indique la part de la variabilité de l'ozone expliquée par la température. On a R² = 0.6 donc T15 semble être un bon prédicteur.

On réalise le graphique représentant le nuage de points (T15, maxO3) et la régression linéaire:

```{r}
ggplot(ozone, aes(x = T15, y = maxO3)) +
  geom_point() +                          # Nuage de points
  geom_smooth(method = "lm", color = "red") + # Droite de régression
  labs(title = "Lien entre Température (15h) et Ozone",
       x = "Température à 15h (T15)",
       y = "Concentration max en Ozone (maxO3)") +
  theme_minimal()
```

### Exercice 3

On veut à présent effectuer une régression linéaire multiple. 

Pour construire ce modèle on va sélectionner les variables suivantes:

- T15, c'est la variable de température avec la plus grosse corrélation avec la variable cible
- Ne12, on a vu que une forte nébulosité faisait baisser la concentration
- max03v, le niveau d'ozone d'un jour dépend en partie de celui de la veille
- Vx15 : le vent joue un rôle crucial

On ne garde que une variable pour chaque type (une température, une nébulosité, un vent) car on a vu que $T9$, $T12$ et $T15$ sont très corrélées entre elles et donc les inclure n'apporterait pas beaucoup de nouvelles infos au modèle et peut même le destabiliser.
C'est aussi car on souhaite respecter le principe de parcimonie: expliquer le maximum de variance avec le minimum de variables possible et donc simplifier le modèle.

De plus on a gardé les variables avec de milieu de journée, car on pense que c'est à ce moment là que les pics de formation d'ozone sont formés.

On effectue donc notre régression multiple:

```{r}
# Modèle de régression linéaire multiple
rl_multiple <- lm(maxO3 ~ T15 + Ne12 + maxO3v + Vx15, data = ozone)

# Affichage du résumé statistique
summary(rl_multiple)
```

Le coefficient de détermination $R^2$ est d'environ 0,735. Cela signifie que notre modèle explique 73,5 % de la variabilité de max03, ce qui est une nette amélioration par rapport au modèle simple (qui tournait autour de 60 %)

T15 et maxO3v ont des p-values très faibles ($< 0,001$), ce qui confirme leur impact majeur et hautement significatif sur max03.
Ne12 a un coefficient négatif (environ -2,35), confirmant que plus le ciel est couvert à midi, plus la concentration d'ozone diminue.
Vx15 est également significatif ($p < 0,05$), montrant que la composante du vent à 15h influence les niveaux de pollution enregistrés.

Ce modèle multiple est bien plus robuste pour la prédiction que le modèle simple car il intègre la synergie entre la température, l'ensoleillement et la persistance temporelle de la pollution.

## Partie 2

Nettoyage de l'environnement:

```{r}
rm(list=ls())
```


### Exercice 1

On commence par charger les données:

```{r}
adclick <- read.csv("Data/AdClick.csv", header = TRUE, sep = ",")
```

Ensuite on effectue quelques statistiques descriptives pour explorer les données, mais avant on va passer en facteur les variables qui ne doivent pas être lues comme numériques:

```{r}
adclick$Click <- as.factor(adclick$Click)
adclick$Gender <- as.factor(adclick$Gender)
```

Maintenant on peut s'intéresser à un summary de notre jeu de données:

```{r}
summary(adclick)
```

Ensuite on veut regarder si l'une de nos variables, à l'air d'avoir une influence sur le fait de cliquer ou non.

```{r}
ggplot(adclick, aes(x = Click, y = Age, fill = Click)) +
  geom_boxplot() +
  labs(title = "Distribution de l'Âge selon le Clic", x = "0=Non, 1=Oui") +
  theme_minimal()
```

Ce boxplot de l'âge montre que le groupe Click = 1 est nettement plus âgé que le groupe Click = 0. Les utilisateurs qui cliquent sur la publicité sont sensiblement plus âgés que ceux qui ne cliquent pas.

De la même manière on regarde pour le salaire:

```{r}
ggplot(adclick, aes(x = Click, y = EstimatedSalary, fill = Click)) +
  geom_boxplot() +
  labs(title = "Distribution du Salaire selon le Clic", y = "Salaire Estimé") +
  theme_minimal()
```

On remarque un net décalage vers le haut pour le groupe qui ont cliqué. La médiane et l'ensemble de la boîte sont situées plus haut que pour le groupe 0.
Les personnes avec un salaire plus élevé ont plus tendance à s'intéresser à la publicité.

```{r}

ggplot(adclick, aes(x = Gender, fill = Click)) +
  geom_bar(position = "fill") + # "fill" pour voir les pourcentages
  labs(title = "Proportion de Clics par Genre", y = "Fréquence") +
  theme_minimal()
```

Les proportions de clics entre les hommes et les femmes sont assez proches, le genre ne semble pas influencer fortement le clic.

### Exercice 2

Nous allons maintenant effectuer une régression logistique pour modéliser la probabilité de cliquer sur la publicité en fonction de l'âge, du salaire et du genre.

On utilise la fonction glm() avec l'argument family = binomial pour spécifier que la variable réponse suit une loi binomiale avec 0 ou 1.

```{r}
# ici on réalise le modèle
reg_log <- glm(Click ~ Age + EstimatedSalary + Gender, 
               data = adclick, 
               family = binomial)

# puis on affiche les résultats
summary(reg_log)
```

L'intercept est très négatif, ce qui signifie que par défaut (âge 0, salaire 0), la probabilité de cliquer est quasi nulle. 
L'age, le coefficient est $0.237 > 0$ et très significatif car $p < 0.001$. Cela confirme notre analyse exploratoire : plus on est âgé, plus la probabilité de cliquer augmente.
Le Salaire: le coefficient est aussi positif et significatif. Un salaire élevé augmente les chances de clic.
Le Genre: le coefficient = 0.33 n'est pas significatif car $p = 0.274 > 0.05$. Cela signifie que le genre n'a pas d'impact réel sur la décision de cliquer une fois qu'on connaît l'âge et le salaire. On pourrait envisager de retirer cette variable pour simplifier le modèle.

Nous allons maintenant observer les erreurs de classifications grâce à une table croisant le clic prédit et le clic effectif
Pour ça on calcule la prédiction des probabilités:

```{r}
prev<-predict(reg_log, type="response")
```

Quand la proba est > 0.5 alors clic = 1 ; sinon clic = 0

```{r}
prev<-as.numeric(prev>0.5)
```

Et maintenant on peut regarder le tableau de fréquence des prédictions 0/1 et l'accuracy de notre modèle:

```{r}
# precision globale
mean(prev==adclick$Click)

table(prev,adclick$Click)
```
Il y a 20 faux positifs (prédits cliqueurs à tort) et 39 faux négatifs (cliqueurs ratés).
Le modèle est performant avec 85% de bonnes prédictions.

### Exercice 3

On souhaite construire un arbre de classification, pour cela on doit charger les librairies :

```{r}
library(rpart)
library(rpart.plot)
```

Ensuite on peut construire notre arbre avec rpart:

```{r}
arbre <- rpart(Click~.,data=adclick,cp=0.02)
```

La librairie rpart.plot permet de résumer l’information de l’arbre et de le représenter graphiquement :

```{r}
rpart.plot(arbre,main="Représentation de l’arbre")
```

Regardons les prédictions et la performance de notre arbre :

```{r}
# Prédictions sous forme de probabilités
pred_arbre<-predict(arbre,data=adclick)

# Prédictions sous forme de variable binaire
pred_arbre_cl<-predict(arbre,data=adclick,type="class")

# Comparaison avec les "vraies" valeurs de Y
table(pred_arbre_cl, adclick$Click)

# Taux de bonnes prédictions
mean(pred_arbre_cl==adclick$Click)
```

L'arbre de classification se révèle être le modèle le plus performant pour ce problème. Avec une précision supérieure à 90%, il surpasse la régression logistique.

# Module 5

